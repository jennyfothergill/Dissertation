%% We could use a little lit review of software development in the science that we can refer to when talking about things that are good or bad that we did or learned from, respectively
\cite{Sholler2019}
How to help newcomers contribute to open source projects
be welcoming!
Have contributing guides, code of conduct
Respond quickly
acknowledge all contributions
"great first issue"

\cite{Sandve2013}
Rule 1: For Every Result, Keep Track of How It Was Produced
like a lab notebook
Rule 2: Avoid Manual Data Manipulation Steps
reduce error, more efficient
Rule 3: Archive the Exact Versions of All External Programs Used
Rule 4: Version Control All Custom Scripts
Rule 5: Record All Intermediate Results, When Possible in Standardized Formats
Rule 6: For Analyses That Include Randomness, Note Underlying Random Seeds
Rule 7: Always Store Raw Data behind Plots
Rule 8: Generate Hierarchical Analysis Output, Allowing Layers of Increasing Detail to Be Inspected
Rule 9: Connect Textual Statements to Underlying Results
Rule 10: Provide Public Access to Scripts, Runs, and Results

\cite{Rule2019a}
How to increase reproducibility when using jupyter notebooks
"perform analyses and combine code, results, and descriptive text in a single “computational narrative” to be read and rerun by others"
1. Tell a story
2. Document the process, not just the results
Show what didn't work and why you chose what you eventually ended up with
3. Use cell divisions to make steps clear
4. Modularize code
5. Record dependencies
6. Use version control
7. Build a pipeline
test
8. Share and explain your data
9. Design your notebooks to be read, run, and explored
10. Advocate for open research

\cite{Maginn2018}
Transport properties. Comparison with experiment is mainly testing the forcefield.
But simulation methodology also matters.
If dynamics matter, then the ensemble may have a greater effect.
Thermostats and barostats can interfere with the dynamics.
Eq 2 is the Einstein eq related to MorphCT
Planckton uses Nose-hoover
Time constant is is time units
In NVE energy must be conserved--can plot energy versus time
Sample length should be longer than correlation time.
If MSD is comparable or larger to L then the molecules have adequately explored the configuration space.
Need unwrapped coords
Recommend using center of mass
Finite size effects: self-diffusivity increases with system size. To account for this a series of larger systems can be simulated and the trend can be observed or a correction factor can be used.
1000 independent samples,
In the diffusive regime, the slope of s log log msd plot is 1
The middle of the msd plot is the best for the linear fit, but there is no good metric for deciding what is the middle.

\cite{Shirts2017}
Reading this to better understand reproducibility study
Comparing results across engines. 
How the charges calculated (whole molecule or fragment) affects the result and this is often not reported.
Compare potential energy, density, enthalpy
started with one file and converted into another and then another
They also used hard cutoff
They found errors due to differences in Colomb's constant!

\cite{Rizzi2020}
Reproducibility considerations for binding energies of ligand/protein. 
comparing engines. 
results suggest the forcefield + partial charges are not enough to ensure reproducibility. 
Berendsen barostat introduces artifacts.
Binding matters for small molecule drug discovery.
Enhanced sampling helps with proteins because there are areas in their configurational space that are difficult to probe with short simulations.
Not all of the engines had the same parameters
Differences between methods impact force field development
- ffs can only be expected to be accurate with the same parameters
Replicates are important to understand the uncertainty in the measurement
Larger and more varied test set gives more information about the method

\cite{Blischak2016}
details how and why to use version control software as a scientist
references for learners, what not to track
address the fear of being scooped--version control gives you a way to prove priority

\cite{Preston-Werner}
semantic versioning guide -- how to decide on the version number for a codebase and what changes should prompt a version number bump

\cite{Gentzkow2014}
gives accounts of what is wrong with coding in the sciences and gives guidelines aimed at fixing these issues. 
Specifically, use version control to store code and data, 
don't overuse abstraction: only for reducing cognitive load and reducing repetition, 
use the available tools like github issues/project boards to manage collaboration and development, documentation, etc.

\cite{Irving2016}
many good references (TODO) 
discusses reproducibility crisis and how published work using software can be more reproducible
why is code not shared: perceived time required to make code ready for publication
1) a more detailed description of the software used, 2) a version controlled and publicly available code repository, and 3) a collection of supplementary log files that captured the data processing steps taken in producing each key result.
recommends SWC

\cite{Noble2009}
could use this as an example of how managing a dataspace is tricky and why we use signac

\cite{Sholler2019}
straightforward guide of how to use github features to streamline code development

\cite{Wilson2014}
Many great references (TODO) good overview of best practices in developing software
1. Write programs for people, not computers.
2. Let the computer do the work.
3. Make incremental changes.
4. Don’t repeat yourself (or others).
5. Plan for mistakes.
6. Optimize software only after it works correctly.
7. Document design and purpose, not mechanics.
8. Collaborate.

\cite{Cito2016}
Docker containers can help solve reproducibility issues

\cite{Shirts2008a}
Details how containers (and even VMs) can be used to aid reproducibility of computational works

\cite{Thompson2020}
many good refs (TODO) outlines mosdef, reproducibility crisis and TRUE

\cite{Jankowski2019}
our perspective paper. many good refs, overview of onboarding and best practices particularly good

\cite{Elofsson2019}
Ten simple rules on how to create open access and reproducible molecular simulations of biological systems
Rule 1: The simulation protocol should be provided
Rule 2: Topology and parameters should be accessible for everyone
Rule 3: Initial coordinate files should be included
Rule 4: Full information about all software used needs to be provided
Rule 5: Simulation results should be deposited in a database
Rule 6: Results should be easy to reproduce
Rule 7: In docking studies, details should be included For
Rule 8: In quantum mechanics calculations, all energies should be included in the results
Rule 9: All sampling-based results should be evaluated using proper statistics
Rule 10: Be Nice

\cite{Galindo-Murillo2015}
Shows that DNA conformation can be obtained using different simulation engines with the same forcefield.
describes how they did it.
some hilarity ensues when they mention hand-editing text files.
"To convert the Amber parameter and topology files into formats appropriate for Anton, the available “amber_topNrst2cms.py” script on the computer anton.psc.edu was used with Desmond [91] to create the needed *.cms file. Note that there is a bug in the “amber_topNrst2cms.py” script that will erroneously assign zero mass to C5′ atoms when converting from Amber topologies, so the generated files were hand-edited to fix the mass and further checked to make sure the resulting *.cms file contained the correct Amber ff99 + parmbsc0 force field parameters."
they use the same engines but on different architectures
SI provides some input scripts but none of the python scripts they mention are available??


\cite{Shirts2008a}
Multistate Bennett acceptance ratio (mbar) estimator 
the timeseries module which automatically identify the equilibrated production region and the subsample method which can filter through a correlated equilibrium sample and return a decorrelated subsample.
