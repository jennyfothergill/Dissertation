\chapter{Introduction}

Molecular simulation gives us the unique ability to peer into the equilibrium structure of atoms and molecules.
The length scale and properties we observe depend on the technique.
Using density functional theory, we can obtain the optimized ground state structure or we can look at the induced shifts in the electron density in the form of charges.
Using molecular dynamics, we can sample the equilibrium structures of the thermodynamic ensemble. 
Analysis techniques can be used to validate simulation results with experiment; for example, structural analysis techniques like grazing-incidence X-Ray scattering (GIXS) allow us to probe the periodic structure, while ultraviolet-visible (UV-Vis) absorption spectroscopy allows us to determine the quantum splitting induced in the molecular orbital energies.
Even without direct comparison to experiment, comparing the relative energies of two different structures using the same model can inform us about which structure is more stable, and the charges on various atoms can help us determine which interactions stabilize that structure.
Comparing the results obtained using different models or engines can inform us of the robustness of reproducibility of our method.

The circular scientific process, consisting of observation, hypothesis, experiment, analysis, and conclusion, often neglects to emphasize a vital extra step which is review and reproduction of the results. 
A Nature study from 2016 found that more than 70\% of researchers had tried and failed to reproduce another scientistâ€™s experiments---over half had failed to reproduce their own past work \cite{Baker2016}.
The experience of trying to replicate a seemingly simple method and failing, is common and discouraging.
Pushing the boundaries of knowledge with new discoveries is important, but it is equally important to validate that these new discoveries are robust and verifiable.

Computer simulation, in which all parameters can be controlled, should be the most straightforward to reproduce and get identical results. 
This is in contrast to a wet-laboratory setting where many unforeseen factors can influence the result.
For example, in the case of measuring fullerene solubility in water, exposure of the experiment to sunlight or ozone in the atmosphere may affect the result \cite{Isaacson2009}.
In a computer simulation, however, no particle or force exists unless it is specified by the user, so theoretically the experiment should be exactly reproducible.
A computational scientist should be able to read a paper in their field, repeat the experiment with the information they gained, and achieve statistically identical results.
Unfortunately, although this goal is simple in theory, many barriers stand between computational scientists and reproducibility.
These struggles can be broken into two categories: those using established code bases and those using bespoke scripts.
When reporting experiments conducted using established codebases, which are stable and distributed open or closed source, the following reproducibility issues can occur:
\begin{itemize}
    \item Lack of documentation of the version of the software or its dependencies. (e.g. BLAS libraries, compiler, etc.)
    \item Potential errors when moving data between software (e.g. improperly handling file type or unit conversions)
    \item Manual editing of files leading to typographical errors
    \item User unfamiliarity with which simulation parameters should be reported
    \item In the case of closed-source software, the simulation details may be obscured; users cannot view the source code and must rely on the documentation being accurate
\end{itemize}
When reporting experiments conducted using custom code, in addition to the above hurdles, the following barriers stand in the way of reproducibility:
\begin{itemize}
    \item Researchers may not provide the source code, whether due to fears of others using the code to publish before them or unfamiliarity with software distribution
    \item If the code is available, it may be non-functioning due to lack of unit-tests and continuous integration
    \item As user developed code is often under revision, inadequate version reporting or complete absence of version control
    \item Poor documentation and examples may make the code base unusable to all but its developers
    \item In the case of a workflow consisting of multiple separate scripts, poor documentation of the process of moving data between scripts or reliance on users manually editing the data may make the logic hard to follow
\end{itemize}
So how can we help computational scientists to make their work more reproducible?
The solution can be divided into three categories: training, tools, and community.

Many scientists are subject matter experts first and learn code development on an as-needed basis, or never have any formal training in software development best practices. 
By investing in initial training, ultimately researchers can save time by working more efficiently.
For example, a new scientist may need to learn bash, git, a programming language, and the specifics of a library or engine that they commonly work with.
Many engines may provide their own tutorials \cite{hoomd_tutorials, Lemkul2019, Lemkul2020}.
Collating some relevant tutorials within a lab (and perhaps creating some additional training materials) can help to standardize the training process and make sure that new researchers are not missing material \cite{notebooktutorials}.
There are many short workshops and references to help newcomers get started, such as Software Carpentry \cite{Wilson2014}.
Software Carpentry is an open-source, community-driven organization which hosts a collection of lessons and conducts in-person and remote workshops to teach basic lab skills for research computing \cite{swc-main}.
Using tools like git can help computational scientists keep track how each result was produced, like a lab notebook \cite{Sandve2013}.
Automation, or using scripts to avoid any manual data manipulation, will help reduce error and boost efficiency and these scripts should be hosted in open version-controlled repositories \cite{Sandve2013}.
For example, if a public version controlled repository is set up from the beginning, when it comes time to publish, no additional work is required to present the code.
Although participation in software-specific training may add to a scientist's upfront workload, the long-term efficiency gain results in saving time.

This training can also help scientists use available tools to increase their productivity while helping to clearly communicate their process to others. 
Version control software, like git, is a great benefit for tracking changes to a code repository, and many repository hosting services, like GitHub or GitLab, also provide issue tracking and project boards which can be used to manage and organize collaboration and development \cite{Gentzkow2014}.
To help users understand the purpose behind a code and how it should be run, there are many tools for creating and hosting documentation such as \texttt{ReadtheDocs} and \texttt{sphinx}.
Providing examples and tutorials can help make the code more usable.
\texttt{Jupyter} notebooks, which contain cells of formatted text and images with runnable code, can be a great way for scientists to show and tell the story of their work \cite{Rule2019a}.
In addition to documentation to help users get the code running, developers can package their code to help users more easily build the software stack necessary to run it.
Providing the exact names and versions of software aids reproducibility; however, providing containers or virtual machines which contain the exact software stack is even better. This saves users the hassle of installation and prevents opportunities for version mismatching \cite{Cito2016, Shirts2008a}.
These tools may add to the cognitive load of a computational scientist, but ultimately they make the code they develop more useable by the community.

By providing adequate training upfront, new scientists can feel more prepared to be part of the open source science community.
Training, inclusivity, and collaboration benefit the software development community \cite{Jankowski2019}.
One reason that scientists may not share their code because they view it as unfinished or messy and don't have time to prepare it for publication or worry about being judged \cite{Irving2016}.
Some scientists may worry that by sharing their work openly, their discoveries will be scooped, but using version control can not only help scientist to track their work but as it provides a signed and timestamped commit---it can be used to prove priority \cite{Blischak2016}. 
But collaboration and participation in a community is vital to helping make sure that computational science is reproducible. 
Community, not prestige, matters to open-source developers \cite{Smirnova2022}.
And there are many simple ways to help newcomers feel welcome from having public contributing guides and codes of conduct to promptly responding to and acknowledging their contributions \cite{Sholler2019}.

Additionally, reproducibility can be aided by not only sharing the source code but also the unedited raw data and analysis methods \cite{Miyakawa2020}. 
By having the complete process---from data collection, to analysis, and even figure creation---be completely automated, transparent, and open, computational results can be the most reproducible and useful to other researchers and society as a whole \cite{Taylor2019, Donoho2009}.
The principles which make computational research transparent, reproducible, useable by others, and extensible (TRUE) \cite{Thompson2020}.
An area where transparency, reproducibility, useability, and extensibility is really brought to the forefront is in my experience developing scientific software with the Molecular Simulation and Design Framework (MoSDeF) team.
I have contributed to the development of open-source packages including \texttt{mBuild}, \texttt{Foyer}, \texttt{Signac}, \texttt{Fresnel}, and \texttt{HOOMD} \cite{mbuild, foyer, signac, fresnel, hoomd}.
These contributions have given me ample opportunity to exercise best software development practices such as writing documentation, using version control, developing unit tests, employing continuous integration (CI), using a fork \& pull workflow, etc.
This experience will help me to be more efficient in my future work.
I've also gained valuable experience working as part of a team.
As my work has been intertwined with existing projects, good collaborative skills were necessary.
The experience has been empowering: submitting changes to reputable code bases has helped me to place myself in the computational science community.
Helping others to realize this feeling for themselves is part of the reason I so strongly believe in the principles of TRUE science.

Throughout my journey as a graduate student, I have grown as a computational scientist.
My research focus shifted part way through, but I would say all of my struggle and efforts have taught me valuable lessons about how to present my work in a way that is most helpful to the next generation of computational scientists.
Broadly, my research has used molecular simulation to determine the morphology of the bulk structure from thermodynamic self-assembly of its constituent parts. 
Learning the intricacies and struggles and what matters to each simulation and how to disseminate and visualize these results.
I then have used various analysis methods to validate these simulated morphologies with those observed in experiment. 
Along the way, I have found my passion in helping to make computational sciences more accessible and reproducible.
My work has focused on developing and using computational tools and workflows to predict and understand self assembly of electronically active molecules.
The resulting four journal publications, two in-preparation manuscripts, two posters, and one conference presentation span all scales from first-principles calculation of electronically active dimers to the creation of new tools for analyzing the long-range periodicities of large scale atomistic and coarse-grain (CG) simulations.

In this thesis I describe projects in which I was both the lead code developer as well as projects where I needed to learn existing code to run  experiments.
An additional focus of this thesis is the work that I have done to make the codebases I have contributed to TRUE in the hopes that they will serve others after I leave.

In my first publication on dimer formation, an investigation of the excitonic splitting observed in cyanine dye dimers was done using density functional theory (DFT) \cite{Fothergill2018}.
By comparing calculated with experimental absorption spectra it was found that the solvent may play a large role in the dimer formation and peak shifting.
At this time I wrote some python code which used singular value decomposition to fit the mostly-planar dye molecules to a vector and a plane. 
I showed a graphic for how this was done in the SI, but did not include any code or any details of what software versions I used. 
The method is not theoretically complex and if I recall correctly I used a function from the \texttt{Scipy} library, but this work would have been more reproducible had the exact code and its dependencies been provided in the supporting information (SI).
At the time, however I was not aware of version control using git or GitHub and instead used Dropbox's history.
I did my best to report all details for the computations I performed although they were done in a proprietary package (Gaussian09), so the source code is not available.
Along with the SI, PDB files for all DFT optimized structures were included, but I did not provide any of the raw input or output files used by Gaussian, nor did I report how I converted these structures to PDB.
Another bespoke code used in the publication, referred to as the KRM code, was developed in Java by Dr Yurke and its theory is given in \citet[Supporting Information]{Cannon2017}, but to my knowledge the source code is not publicly available or under version control, so I could not report its version or give instructions for how this work could be reproduced.
This critique of my past work is by no means a condemnation of it; my effort and the guidance I received from my collaborators were an invaluable learning experience.
Learning to use git and GitHub is an initial barrier which adds to cognitive load---a novice computational scientist needs to learn the theory and background of their method, how to navigate the shell, how to use a new software, and perhaps a scripting language, so although using version control is a vital part of writing reproducible scientific code, initially learning how to use it may seem like an unwanted additional hurdle.
I was also somewhat ignorant of what details I should report---at the time I don't think I knew how to determine what version of python or java or which additional libraries I was using.
I performed all simulations under the guidance of Drs. Li and Yurke, created all figures, and wrote all stages of draft this paper with review feedback from Dr Li, Dr Knowlton, and Dr. Yurke.

In my second publication, an investigation of the supramolecular interactions which guide crystal self-assembly in diphenylurea transition metal complexes was done using DFT \cite{Millard2019a}.
I contributed DFT energies and optimized structures under the guidance of Dr King.

My contribution to the lab's perspective paper was mostly about my on-boarding experience trying to reproduce an existing model; a task which should be very straightforward but, which any computational scientist can attest, is far from simple \cite{Jankowski2019}.

In my first poster contribution as Boise State University's 2020 Research Computing days, I presented a tool called GIXStapose, an interactive structure viewer alongside its simulated diffraction pattern \cite{gixstapose}. And this work was also presented at the 2020 SciPy conference \cite{gixstapose, scipy2020}.

My third publication was a thorough investigation of the effect of para-substituents to the crystal structure of diphenylurea molecules. I contributed DFT calculations, data analysis, writing in the initial draft, and images made in Figures 3, 4, 5, 6, and 8 \cite{Fothergill2021}.

In a talk given at the 2021 American Institute of Chemical Engineers conference, I presented how MoSDeF tools can be used to update an older workflow, making it more TRUE. 
I will elaborate on that workflow and its use to validate that the structures self-assembled by P3HT show the same structures and temperature and solvent dependency as in prior work using a different model.

Finally, I will summarize my currently unpublished effort in a multi-university collaborative study to use MoSDeF tools reproducibly.

Through my work spanning all scales of molecular simulation---from implementing details in MD engines, to creating analysis software, to performing DFT calculations of single molecules, simulations of collections of molecules, and performing thousands of  simulations of collections of molecules across thermodynamic state space---the need and utility of  pipelines and practices emphasizing transferability, reproducibility, useability, and extensibility has only been reinforced.

%% what can reproducibility look like in molecular simulation